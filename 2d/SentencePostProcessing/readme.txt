https://github.com/google-research/bert
BERT Transformer concept is used
bert-base-uncased pretrained model is used
12-layer, 768-hidden, 12-heads, 110M parameters
84.55% evaluatiion accuracy